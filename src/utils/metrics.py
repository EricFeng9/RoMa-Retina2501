import torch
import cv2
import numpy as np
from collections import OrderedDict
from loguru import logger
from kornia.geometry.epipolar import numeric
from kornia.geometry.conversions import convert_points_to_homogeneous


# --- METRICS ---

def relative_pose_error(T_0to1, R, t, ignore_gt_t_thr=0.0):
    # angle error between 2 vectors
    t_gt = T_0to1[:3, 3]
    n = np.linalg.norm(t) * np.linalg.norm(t_gt)
    t_err = np.rad2deg(np.arccos(np.clip(np.dot(t, t_gt) / n, -1.0, 1.0)))
    t_err = np.minimum(t_err, 180 - t_err)  # handle E ambiguity
    if np.linalg.norm(t_gt) < ignore_gt_t_thr:  # pure rotation is challenging
        t_err = 0

    # angle error between 2 rotation matrices
    R_gt = T_0to1[:3, :3]
    cos = (np.trace(np.dot(R.T, R_gt)) - 1) / 2
    cos = np.clip(cos, -1., 1.)  # handle numercial errors
    R_err = np.rad2deg(np.abs(np.arccos(cos)))

    return t_err, R_err


def symmetric_epipolar_distance(pts0, pts1, E, K0, K1):
    """Squared symmetric epipolar distance.
    This can be seen as a biased estimation of the reprojection error.
    Args:
        pts0 (torch.Tensor): [N, 2]
        E (torch.Tensor): [3, 3]
    """
    pts0 = (pts0 - K0[[0, 1], [2, 2]][None]) / K0[[0, 1], [0, 1]][None]
    pts1 = (pts1 - K1[[0, 1], [2, 2]][None]) / K1[[0, 1], [0, 1]][None]
    pts0 = convert_points_to_homogeneous(pts0)
    pts1 = convert_points_to_homogeneous(pts1)

    Ep0 = pts0 @ E.T  # [N, 3]
    p1Ep0 = torch.sum(pts1 * Ep0, -1)  # [N,]
    Etp1 = pts1 @ E  # [N, 3]

    d = p1Ep0**2 * (1.0 / (Ep0[:, 0]**2 + Ep0[:, 1]**2) + 1.0 / (Etp1[:, 0]**2 + Etp1[:, 1]**2))  # N
    return d


def compute_symmetrical_epipolar_errors(data):
    """ 
    Update:
        data (dict):{"epi_errs": [M]}
    """
    dataset_name = data['dataset_name'][0].lower()
    if dataset_name == 'multimodal' or dataset_name == 'realdataset':
        return compute_homography_reprojection_errors(data)
    
    Tx = numeric.cross_product_matrix(data['T_0to1'][:, :3, 3])
    E_mat = Tx @ data['T_0to1'][:, :3, :3]

    m_bids = data['m_bids']
    pts0 = data['mkpts0_f']
    pts1 = data['mkpts1_f']

    epi_errs = []
    for bs in range(Tx.size(0)):
        mask = m_bids == bs
        epi_errs.append(
            symmetric_epipolar_distance(pts0[mask], pts1[mask], E_mat[bs], data['K0'][bs], data['K1'][bs]))
    epi_errs = torch.cat(epi_errs, dim=0)

    data.update({'epi_errs': epi_errs})


def compute_homography_reprojection_errors(data):
    """
    è®¡ç®—åŸºäºå•åº”çŸ©é˜µçš„é‡æŠ•å½±è¯¯å·® (é’ˆå¯¹ MultiModal æ•°æ®é›†)
    Update:
        data (dict):{"epi_errs": [M]}
    """
    m_bids = data['m_bids']
    pts0 = data['mkpts0_f']
    pts1 = data['mkpts1_f']
    H_0to1 = data['T_0to1'] # [N, 3, 3]

    epi_errs = []
    for bs in range(H_0to1.size(0)):
        mask = m_bids == bs
        if mask.sum() == 0:
            continue
        
        # å°† pts0 æŠ•å½±åˆ° image1
        p0 = pts0[mask]
        p0_h = torch.cat([p0, torch.ones_like(p0[:, :1])], dim=-1)
        p0_warped_h = (H_0to1[bs] @ p0_h.t()).t()
        p0_warped = p0_warped_h[:, :2] / (p0_warped_h[:, 2:] + 1e-7)
        
        # è®¡ç®— L2 è·ç¦»ä½œä¸ºè¯¯å·®
        err = torch.norm(p0_warped - pts1[mask], dim=-1)
        epi_errs.append(err)
        
    if len(epi_errs) == 0:
        data.update({'epi_errs': torch.tensor([], device=pts0.device)})
    else:
        data.update({'epi_errs': torch.cat(epi_errs, dim=0)})


def estimate_pose(kpts0, kpts1, K0, K1, thresh, conf=0.99999):
    if len(kpts0) < 5:
        return None
    # normalize keypoints
    kpts0 = (kpts0 - K0[[0, 1], [2, 2]][None]) / K0[[0, 1], [0, 1]][None]
    kpts1 = (kpts1 - K1[[0, 1], [2, 2]][None]) / K1[[0, 1], [0, 1]][None]

    # normalize ransac threshold
    ransac_thr = thresh / np.mean([K0[0, 0], K1[1, 1], K0[0, 0], K1[1, 1]])

    # compute pose with cv2
    E, mask = cv2.findEssentialMat(
        kpts0, kpts1, np.eye(3), threshold=ransac_thr, prob=conf, method=cv2.RANSAC)
    if E is None:
        print("\nE is None while trying to recover pose.\n")
        return None

    # recover pose from E
    best_num_inliers = 0
    ret = None
    for _E in np.split(E, len(E) / 3):
        n, R, t, _ = cv2.recoverPose(_E, kpts0, kpts1, np.eye(3), 1e9, mask=mask)
        if n > best_num_inliers:
            ret = (R, t[:, 0], mask.ravel() > 0)
            best_num_inliers = n

    return ret


def compute_pose_errors(data, config):
    """ 
    Update:
        data (dict):{
            "R_errs" List[float]: [N]
            "t_errs" List[float]: [N]
            "inliers" List[np.ndarray]: [N]
        }
    """
    dataset_name = data['dataset_name'][0].lower()
    if dataset_name == 'multimodal' or dataset_name == 'realdataset':
        return compute_homography_errors(data, config)
    
    pixel_thr = config.TRAINER.RANSAC_PIXEL_THR  # 0.5
    conf = config.TRAINER.RANSAC_CONF  # 0.99999
    data.update({'R_errs': [], 't_errs': [], 'inliers': []})

    m_bids = data['m_bids'].cpu().numpy()
    pts0 = data['mkpts0_f'].cpu().numpy()
    pts1 = data['mkpts1_f'].cpu().numpy()
    K0 = data['K0'].cpu().numpy()
    K1 = data['K1'].cpu().numpy()
    T_0to1 = data['T_0to1'].cpu().numpy()

    for bs in range(K0.shape[0]):
        mask = m_bids == bs
        ret = estimate_pose(pts0[mask], pts1[mask], K0[bs], K1[bs], pixel_thr, conf=conf)

        if ret is None:
            data['R_errs'].append(np.inf)
            data['t_errs'].append(np.inf)
            data['inliers'].append(np.array([]).astype(bool))
        else:
            R, t, inliers = ret
            t_err, R_err = relative_pose_error(T_0to1[bs], R, t, ignore_gt_t_thr=0.0)
            data['R_errs'].append(R_err)
            data['t_errs'].append(t_err)
            data['inliers'].append(inliers)


def spatial_binning(pts0, pts1, img_size, grid_size=4, top_n=20, conf=None):
    """
    ã€æ–¹æ¡ˆ B æ”¹è¿›ã€‘ç©ºé—´å‡åŒ€åŒ– (Spatial Binning)
    å°†å›¾åƒåˆ’åˆ†ä¸º grid_size x grid_size çš„ç½‘æ ¼ï¼Œæ¯ä¸ªç½‘æ ¼å†…æœ€å¤šä¿ç•™ Top-N ä¸ªåŒ¹é…ç‚¹ã€‚
    """
    h, w = img_size
    cell_h = h / grid_size
    cell_w = w / grid_size
    
    selected_indices = []
    
    # æ¨¡æ‹Ÿç½‘æ ¼
    grid = [[] for _ in range(grid_size * grid_size)]
    
    for i, pt in enumerate(pts0):
        gx = min(int(pt[0] / cell_w), grid_size - 1)
        gy = min(int(pt[1] / cell_h), grid_size - 1)
        grid[gy * grid_size + gx].append(i)
        
    for cell_indices in grid:
        if len(cell_indices) == 0:
            continue
        
        if conf is not None:
            # æŒ‰ç½®ä¿¡åº¦æ’åº
            cell_indices = sorted(cell_indices, key=lambda idx: conf[idx], reverse=True)
            
        selected_indices.extend(cell_indices[:top_n])
        
    return np.array(selected_indices)


def compute_homography_errors(data, config):
    """
    è®¡ç®—å•åº”çŸ©é˜µä¼°è®¡è¯¯å·® (é’ˆå¯¹ MultiModal æ•°æ®é›†)
    """
    data.update({'R_errs': [], 't_errs': [], 'inliers': [], 'H_est': []})
    
    m_bids = data['m_bids'].cpu().numpy()
    pts0 = data['mkpts0_f'].cpu().numpy()
    pts1 = data['mkpts1_f'].cpu().numpy()
    H_gt = data['T_0to1'].cpu().numpy()
    mconf = data.get('mconf')
    if mconf is not None:
        mconf = mconf.cpu().numpy()
    
    for bs in range(H_gt.shape[0]):
        mask = m_bids == bs
        num_matches = np.sum(mask)
        
        if num_matches < 4:
            from loguru import logger
            logger.warning(f"âš ï¸ Batch {bs}: åŒ¹é…ç‚¹æ•°ä¸è¶³ ({num_matches} < 4)")
            data['R_errs'].append(np.inf)
            data['t_errs'].append(np.inf)
            data['inliers'].append(np.array([]).astype(bool))
            data['H_est'].append(np.eye(3))
            continue
            
        # ä¼°è®¡å•åº”çŸ©é˜µ (å¯¹åº” plan.md ç¬¬å››é˜¶æ®µ: å‡ ä½•ä¼°è®¡)
        # ã€æ–¹æ¡ˆ B æ”¹è¿›ã€‘è¿›è¡Œç©ºé—´å‡åŒ€åŒ– (Spatial Binning)
        img_size = data['image0'].shape[2:]
        pts0_batch = pts0[mask]
        pts1_batch = pts1[mask]
        mconf_batch = mconf[mask] if mconf is not None else None
        
        bin_indices = spatial_binning(pts0_batch, pts1_batch, img_size, grid_size=4, top_n=20, conf=mconf_batch)
        
        from loguru import logger
        logger.info(f"ğŸ” Batch {bs}: æ€»åŒ¹é…ç‚¹={num_matches}, Spatial Binningå={len(bin_indices)}")
        
        if len(bin_indices) >= 4:
            pts0_ransac = pts0_batch[bin_indices]
            pts1_ransac = pts1_batch[bin_indices]
            H_est, inliers = cv2.findHomography(pts0_ransac, pts1_ransac, cv2.RANSAC, config.TRAINER.RANSAC_PIXEL_THR)
        else:
            H_est, inliers = cv2.findHomography(pts0_batch, pts1_batch, cv2.RANSAC, config.TRAINER.RANSAC_PIXEL_THR)
        
        if H_est is None:
            # ã€è°ƒè¯•ã€‘RANSAC å¤±è´¥ï¼Œè®°å½•åŸå› 
            from loguru import logger
            logger.warning(f"âš ï¸ Batch {bs}: RANSAC è¿”å› None (åŒ¹é…ç‚¹æ•°: {len(bin_indices) if len(bin_indices) >= 4 else num_matches})")
            data['R_errs'].append(np.inf)
            data['t_errs'].append(np.inf)
            data['inliers'].append(np.array([]).astype(bool))
            data['H_est'].append(np.eye(3))
        else:
            # ã€è°ƒè¯•ã€‘æ£€æŸ¥ inliers æ•°é‡å’ŒçŸ©é˜µçŠ¶æ€
            from loguru import logger
            num_inliers = np.sum(inliers.ravel() > 0) if inliers is not None else 0
            is_identity = np.allclose(H_est, np.eye(3), atol=1e-3)
            
            logger.info(f"âœ… Batch {bs}: RANSAC æˆåŠŸ, inliers={num_inliers}/{len(bin_indices) if len(bin_indices) >= 4 else num_matches}, H_estæ˜¯å¦å•ä½çŸ©é˜µ={is_identity}")
            
            if is_identity:
                logger.warning(f"âš ï¸ Batch {bs}: H_est æ¥è¿‘å•ä½çŸ©é˜µ! è¿™ä¸æ­£å¸¸!")
                logger.warning(f"   pts0 èŒƒå›´: [{pts0_batch[:, 0].min():.1f}, {pts0_batch[:, 0].max():.1f}] x [{pts0_batch[:, 1].min():.1f}, {pts0_batch[:, 1].max():.1f}]")
                logger.warning(f"   pts1 èŒƒå›´: [{pts1_batch[:, 0].min():.1f}, {pts1_batch[:, 0].max():.1f}] x [{pts1_batch[:, 1].min():.1f}, {pts1_batch[:, 1].max():.1f}]")
            elif num_inliers < 30:
                logger.warning(f"âš ï¸ Batch {bs}: Inliers æ•°é‡è¾ƒå°‘ ({num_inliers}), å¯èƒ½å¯¼è‡´é…å‡†è´¨é‡å·®")
            
            # å¯¹äºçœ¼åº•å›¾åƒé…å‡†ï¼Œæˆ‘ä»¬å°† R_errs è®¾ä¸º 0
            # å°† t_errs è®¾ä¸º Corner Errorï¼Œç”¨äº AUC è®¡ç®— (å¯¹åº” MegaDepth/LoFTR çš„æ ‡å‡†è¯„æµ‹æ–¹å¼)
            data['R_errs'].append(0.0)
            
            # é‡è¦ä¿®å¤ï¼šè®¡ç®— Corner Error (ä¼°è®¡ H ä¸ çœŸå€¼ H ä¹‹é—´çš„åå·®)
            # è€Œä¸æ˜¯è®¡ç®— H_est åœ¨å…¶è‡ªèº«å†…ç‚¹ä¸Šçš„æ®‹å·®
            h, w = data['image0'].shape[2:]
            corners = np.array([[0, 0], [w-1, 0], [w-1, h-1], [0, h-1]], dtype=np.float32)
            corners_h = np.concatenate([corners, np.ones((4, 1))], axis=-1)
            
            # ä½¿ç”¨çœŸå€¼ H æŠ•å½±å¾—åˆ° GT åæ ‡
            corners_gt_h = (H_gt[bs] @ corners_h.T).T
            corners_gt = corners_gt_h[:, :2] / (corners_gt_h[:, 2:] + 1e-7)
            
            # ä½¿ç”¨ä¼°è®¡ H æŠ•å½±å¾—åˆ°é¢„æµ‹åæ ‡
            corners_est_h = (H_est @ corners_h.T).T
            corners_est = corners_est_h[:, :2] / (corners_est_h[:, 2:] + 1e-7)
            
            # è®¡ç®—å¹³å‡è§’ç‚¹è¯¯å·®
            err = np.mean(np.linalg.norm(corners_est - corners_gt, axis=-1))
            
            data['t_errs'].append(err)
            data['inliers'].append(inliers.ravel() > 0)
            data['H_est'].append(H_est)


# --- METRIC AGGREGATION ---

def error_auc(errors, thresholds):
    """
    Args:
        errors (list): [N,]
        thresholds (list)
    """
    errors = [0] + sorted(list(errors))
    recall = list(np.linspace(0, 1, len(errors)))

    aucs = []
    thresholds = [5, 10, 20]
    for thr in thresholds:
        last_index = np.searchsorted(errors, thr)
        y = recall[:last_index] + [recall[last_index-1]]
        x = errors[:last_index] + [thr]
        aucs.append(np.trapz(y, x) / thr)

    return {f'auc@{t}': auc for t, auc in zip(thresholds, aucs)}


def epidist_prec(errors, thresholds, ret_dict=False):
    precs = []
    for thr in thresholds:
        prec_ = []
        for errs in errors:
            correct_mask = errs < thr
            prec_.append(np.mean(correct_mask) if len(correct_mask) > 0 else 0)
        precs.append(np.mean(prec_) if len(prec_) > 0 else 0)
    if ret_dict:
        return {f'prec@{t:.0e}': prec for t, prec in zip(thresholds, precs)}
    else:
        return precs


def aggregate_metrics(metrics, epi_err_thr=5e-4):
    """ Aggregate metrics for the whole dataset:
    (This method should be called once per dataset)
    1. AUC of the pose error (angular) at the threshold [5, 10, 20]
    2. Mean matching precision at the threshold 5e-4(ScanNet), 1e-4(MegaDepth)
    """
    # filter duplicates
    unq_ids = OrderedDict((iden, id) for id, iden in enumerate(metrics['identifiers']))
    unq_ids = list(unq_ids.values())
    logger.info(f'Aggregating metrics over {len(unq_ids)} unique items...')

    # pose auc
    angular_thresholds = [5, 10, 20]
    pose_errors = np.max(np.stack([metrics['R_errs'], metrics['t_errs']]), axis=0)[unq_ids]
    aucs = error_auc(pose_errors, angular_thresholds)  # (auc@5, auc@10, auc@20)

    # matching precision
    dist_thresholds = [epi_err_thr]
    precs = epidist_prec(np.array(metrics['epi_errs'], dtype=object)[unq_ids], dist_thresholds, True)  # (prec@err_thr)

    return {**aucs, **precs}
